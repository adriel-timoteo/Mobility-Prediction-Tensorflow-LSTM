{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>DATA CLEANING</h2>\n",
    "\n",
    "Script ini digunakan untuk membersihkan data dari GrabPosisi jadi data yang lebih mudah untuk dimengerti dan dipelajari oleh model LSTMnya nanti. Proses Cleaning Data ini meliputi:\n",
    "- Mengurutkan Data berdasarkan trj_id lalu pingtimestamp agar data terurut dengan baik\n",
    "- Mengelompokkan data dan menjadikannya Pandas Multi Index Dataframe dengan trj_id sebagai index, untuk mempermudah pemisahan trajectories nantinya\n",
    "- Me-resample data (Data awal memiliki sampling rate 1 kali per **detik**, namun beberapa kali jarak antar ping bisa lebih dari 1 detik). Setelah resampling, data memiliki sampling rate 1 kali per **menit** dan konstan (Tidak ada jarak antar ping yang berbeda)\n",
    "<br/>\n",
    "Hasil dari data cleaning ini sudah diexport menjadi clean_data.csv, sehingga script ini tidak perlu dijalankan kecuali ingin mengubah proses data cleaning di atas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import libraries\n",
    "import pandas as pd\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding file: ..\\GrabPosisi Dataset\\part-00000-8bbff892-97d2-4011-9961-703e38972569.c000.snappy.parquet\n",
      "Adding file: ..\\GrabPosisi Dataset\\part-00001-8bbff892-97d2-4011-9961-703e38972569.c000.snappy.parquet\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m parquet_file \u001b[38;5;129;01min\u001b[39;00m data_dir\u001b[38;5;241m.\u001b[39mglob(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAdding file: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparquet_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 6\u001b[0m     full_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([full_df, \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparquet_file\u001b[49m\u001b[43m)\u001b[49m], ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\adrie\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\io\\parquet.py:670\u001b[0m, in \u001b[0;36mread_parquet\u001b[1;34m(path, engine, columns, storage_options, use_nullable_dtypes, dtype_backend, filesystem, filters, **kwargs)\u001b[0m\n\u001b[0;32m    667\u001b[0m     use_nullable_dtypes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    668\u001b[0m check_dtype_backend(dtype_backend)\n\u001b[1;32m--> 670\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m impl\u001b[38;5;241m.\u001b[39mread(\n\u001b[0;32m    671\u001b[0m     path,\n\u001b[0;32m    672\u001b[0m     columns\u001b[38;5;241m=\u001b[39mcolumns,\n\u001b[0;32m    673\u001b[0m     filters\u001b[38;5;241m=\u001b[39mfilters,\n\u001b[0;32m    674\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[0;32m    675\u001b[0m     use_nullable_dtypes\u001b[38;5;241m=\u001b[39muse_nullable_dtypes,\n\u001b[0;32m    676\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m    677\u001b[0m     filesystem\u001b[38;5;241m=\u001b[39mfilesystem,\n\u001b[0;32m    678\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    679\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\adrie\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\io\\parquet.py:272\u001b[0m, in \u001b[0;36mPyArrowImpl.read\u001b[1;34m(self, path, columns, filters, use_nullable_dtypes, dtype_backend, storage_options, filesystem, **kwargs)\u001b[0m\n\u001b[0;32m    265\u001b[0m path_or_handle, handles, filesystem \u001b[38;5;241m=\u001b[39m _get_path_or_handle(\n\u001b[0;32m    266\u001b[0m     path,\n\u001b[0;32m    267\u001b[0m     filesystem,\n\u001b[0;32m    268\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[0;32m    269\u001b[0m     mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    270\u001b[0m )\n\u001b[0;32m    271\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 272\u001b[0m     pa_table \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi\u001b[38;5;241m.\u001b[39mparquet\u001b[38;5;241m.\u001b[39mread_table(\n\u001b[0;32m    273\u001b[0m         path_or_handle,\n\u001b[0;32m    274\u001b[0m         columns\u001b[38;5;241m=\u001b[39mcolumns,\n\u001b[0;32m    275\u001b[0m         filesystem\u001b[38;5;241m=\u001b[39mfilesystem,\n\u001b[0;32m    276\u001b[0m         filters\u001b[38;5;241m=\u001b[39mfilters,\n\u001b[0;32m    277\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    278\u001b[0m     )\n\u001b[0;32m    279\u001b[0m     result \u001b[38;5;241m=\u001b[39m pa_table\u001b[38;5;241m.\u001b[39mto_pandas(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mto_pandas_kwargs)\n\u001b[0;32m    281\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m manager \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\adrie\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pyarrow\\parquet\\core.py:3003\u001b[0m, in \u001b[0;36mread_table\u001b[1;34m(source, columns, use_threads, metadata, schema, use_pandas_metadata, read_dictionary, memory_map, buffer_size, partitioning, filesystem, filters, use_legacy_dataset, ignore_prefixes, pre_buffer, coerce_int96_timestamp_unit, decryption_properties, thrift_string_size_limit, thrift_container_size_limit)\u001b[0m\n\u001b[0;32m   2992\u001b[0m         \u001b[38;5;66;03m# TODO test that source is not a directory or a list\u001b[39;00m\n\u001b[0;32m   2993\u001b[0m         dataset \u001b[38;5;241m=\u001b[39m ParquetFile(\n\u001b[0;32m   2994\u001b[0m             source, metadata\u001b[38;5;241m=\u001b[39mmetadata, read_dictionary\u001b[38;5;241m=\u001b[39mread_dictionary,\n\u001b[0;32m   2995\u001b[0m             memory_map\u001b[38;5;241m=\u001b[39mmemory_map, buffer_size\u001b[38;5;241m=\u001b[39mbuffer_size,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3000\u001b[0m             thrift_container_size_limit\u001b[38;5;241m=\u001b[39mthrift_container_size_limit,\n\u001b[0;32m   3001\u001b[0m         )\n\u001b[1;32m-> 3003\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_threads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_threads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3004\u001b[0m \u001b[43m                        \u001b[49m\u001b[43muse_pandas_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_pandas_metadata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3006\u001b[0m warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   3007\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPassing \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muse_legacy_dataset=True\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m to get the legacy behaviour is \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3008\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdeprecated as of pyarrow 8.0.0, and the legacy implementation will \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3009\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbe removed in a future version.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   3010\u001b[0m     \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m   3012\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ignore_prefixes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\adrie\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pyarrow\\parquet\\core.py:2631\u001b[0m, in \u001b[0;36m_ParquetDatasetV2.read\u001b[1;34m(self, columns, use_threads, use_pandas_metadata)\u001b[0m\n\u001b[0;32m   2623\u001b[0m         index_columns \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m   2624\u001b[0m             col \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m _get_pandas_index_columns(metadata)\n\u001b[0;32m   2625\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(col, \u001b[38;5;28mdict\u001b[39m)\n\u001b[0;32m   2626\u001b[0m         ]\n\u001b[0;32m   2627\u001b[0m         columns \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   2628\u001b[0m             \u001b[38;5;28mlist\u001b[39m(columns) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(index_columns) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mset\u001b[39m(columns))\n\u001b[0;32m   2629\u001b[0m         )\n\u001b[1;32m-> 2631\u001b[0m table \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_table\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2632\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mfilter\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_filter_expression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2633\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_threads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_threads\u001b[49m\n\u001b[0;32m   2634\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2636\u001b[0m \u001b[38;5;66;03m# if use_pandas_metadata, restore the pandas metadata (which gets\u001b[39;00m\n\u001b[0;32m   2637\u001b[0m \u001b[38;5;66;03m# lost if doing a specific `columns` selection in to_table)\u001b[39;00m\n\u001b[0;32m   2638\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_pandas_metadata:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Read Data\n",
    "data_dir = Path('../GrabPosisi Dataset')\n",
    "full_df = pd.DataFrame()\n",
    "for parquet_file in data_dir.glob(\"*.parquet\"):\n",
    "    print(f\"Adding file: {parquet_file}\")\n",
    "    full_df = pd.concat([full_df, pd.read_parquet(parquet_file)], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         trj_id  pingtimestamp    rawlat      rawlng  speed  bearing\n",
      "29091989      1     1554992255 -6.197622  106.769017   5.58      180\n",
      "29074360      1     1554992256 -6.197667  106.769007   5.33      177\n",
      "10694992      1     1554992257 -6.197713  106.769012   5.43      177\n",
      "10676471      1     1554992258 -6.197764  106.769020   5.84      178\n",
      "22141074      1     1554992259 -6.197809  106.769018   5.28      179\n",
      "...         ...            ...       ...         ...    ...      ...\n",
      "19921434   9999     1555822630 -6.178844  106.841960   0.00        0\n",
      "7604364    9999     1555822631 -6.178844  106.841960   0.00        0\n",
      "25980783   9999     1555822632 -6.178844  106.841961   0.00        0\n",
      "14850195   9999     1555822634 -6.178845  106.841963   0.00        0\n",
      "19919728   9999     1555822635 -6.178845  106.841964   0.00        0\n",
      "\n",
      "[55988420 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "# Remove unnecessary columns (driving_mode, osname, accuracy)\n",
    "train_df = full_df[['trj_id', 'pingtimestamp', 'rawlat', 'rawlng', 'speed', 'bearing']]\n",
    "\n",
    "# Sort by trj_id and then pingtimestamp\n",
    "train_df = train_df.sort_values(by=[\"trj_id\", \"pingtimestamp\"])\n",
    "\n",
    "print(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         trj_id  pingtimestamp    rawlat      rawlng  speed  bearing\n",
      "29091989      1     1554992255 -6.197622  106.769017   5.58      180\n",
      "29074360      1     1554992256 -6.197667  106.769007   5.33      177\n",
      "10694992      1     1554992257 -6.197713  106.769012   5.43      177\n",
      "10676471      1     1554992258 -6.197764  106.769020   5.84      178\n",
      "22141074      1     1554992259 -6.197809  106.769018   5.28      179\n",
      "...         ...            ...       ...         ...    ...      ...\n",
      "38985733   9999     1555822576 -6.178897  106.841986   1.71      348\n",
      "7600735    9999     1555822577 -6.178879  106.841984   1.76        1\n",
      "40999024   9999     1555822578 -6.178863  106.841984   1.50        7\n",
      "31725161   9999     1555822579 -6.178853  106.841985   0.83       15\n",
      "25966058   9999     1555822609 -6.178857  106.841958   0.22      348\n",
      "\n",
      "[53028538 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "# Remove data with 0 values\n",
    "cleaned_df = train_df.dropna(subset=[\"speed\", \"pingtimestamp\", \"rawlat\", \"rawlng\"]).query(\"speed > 0 and pingtimestamp != 0 and rawlat != 0 and rawlng != 0\")\n",
    "\n",
    "print(cleaned_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                pingtimestamp    rawlat      rawlng      speed     bearing\n",
      "trj_id                                                                    \n",
      "1      0  2019-04-11 14:17:00 -6.198042  106.769008   4.322800  179.920000\n",
      "       1  2019-04-11 14:18:00 -6.200972  106.769202   8.014167  173.233333\n",
      "       2  2019-04-11 14:19:00 -6.205394  106.769768  10.116136  171.477273\n",
      "       3  2019-04-11 14:20:00 -6.210496  106.771217   9.307667  156.683333\n",
      "       4  2019-04-11 14:21:00 -6.214969  106.773830  10.103333  139.777778\n",
      "...                       ...       ...         ...        ...         ...\n",
      "9999   32 2019-04-21 04:52:00 -6.187751  106.845707  10.584667  329.600000\n",
      "       33 2019-04-21 04:53:00 -6.184123  106.843546   4.508780  324.512195\n",
      "       34 2019-04-21 04:54:00 -6.182706  106.842869   2.776724  287.137931\n",
      "       35 2019-04-21 04:55:00 -6.180504  106.842337   5.244333  326.850000\n",
      "       36 2019-04-21 04:56:00 -6.179029  106.841998   2.330952  231.095238\n",
      "\n",
      "[1221233 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "# Group data by trj_id\n",
    "cleaned_df['pingtimestamp'] = pd.to_datetime(cleaned_df['pingtimestamp'], unit='s') # Convert unix timestamp to Pandas DateTime\n",
    "grouped_data = cleaned_df.groupby('trj_id') # Group the dataframe by trj_id\n",
    "\n",
    "# Resample by minute\n",
    "def resample_trajectory(data):\n",
    "  \"\"\"\n",
    "  First resample the data to every one minute (1T) by making the pingtimestamp an index (turning it to a pandas multi index dataframe),\n",
    "  then linear interpolate tha data (if a data is null, take average of the previous and next data),\n",
    "  then convert the pingtimestamp from index back to column.\n",
    "\n",
    "  The result is a pandas multi index dataframe with the trj_id as index, and pingtimestamp, rawlat, rawlng, speed, bearing as columns.\n",
    "  \"\"\"\n",
    "  resampled_data = data.resample('1T', on='pingtimestamp')['rawlat', 'rawlng', 'speed', 'bearing'].mean().interpolate('linear').reset_index()\n",
    "  return resampled_data\n",
    "\n",
    "resampled_data = grouped_data.apply(resample_trajectory)\n",
    "\n",
    "print(resampled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             rawlat      rawlng      speed     bearing  hour_of_day  \\\n",
      "trj_id                                                                \n",
      "1      0  -6.198042  106.769008   4.322800  179.920000           14   \n",
      "       1  -6.200972  106.769202   8.014167  173.233333           14   \n",
      "       2  -6.205394  106.769768  10.116136  171.477273           14   \n",
      "       3  -6.210496  106.771217   9.307667  156.683333           14   \n",
      "       4  -6.214969  106.773830  10.103333  139.777778           14   \n",
      "...             ...         ...        ...         ...          ...   \n",
      "9999   32 -6.187751  106.845707  10.584667  329.600000            4   \n",
      "       33 -6.184123  106.843546   4.508780  324.512195            4   \n",
      "       34 -6.182706  106.842869   2.776724  287.137931            4   \n",
      "       35 -6.180504  106.842337   5.244333  326.850000            4   \n",
      "       36 -6.179029  106.841998   2.330952  231.095238            4   \n",
      "\n",
      "           day_of_week  \n",
      "trj_id                  \n",
      "1      0             3  \n",
      "       1             3  \n",
      "       2             3  \n",
      "       3             3  \n",
      "       4             3  \n",
      "...                ...  \n",
      "9999   32            6  \n",
      "       33            6  \n",
      "       34            6  \n",
      "       35            6  \n",
      "       36            6  \n",
      "\n",
      "[1221233 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "# Make hour_of_day and day_of_week from pingtimestamp (Used so the model can better predict user movement from time of day and day of week)\n",
    "resampled_data['hour_of_day'] = resampled_data['pingtimestamp'].dt.hour\n",
    "resampled_data['day_of_week'] = resampled_data['pingtimestamp'].dt.dayofweek\n",
    "resampled_data = resampled_data.drop('pingtimestamp', axis=1) # Drop pingtimestamp as it is no longer needed\n",
    "print(resampled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export data to CSV\n",
    "resampled_data.to_csv('clean_data.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
