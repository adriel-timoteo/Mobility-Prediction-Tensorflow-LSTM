{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>FINAL DATA PREPROCESSING & MODEL TRAINING</h2>\n",
    "\n",
    "Script ini digunakan untuk mempreprocess data menjadi data final yang dapat digunakan untuk training model, dan training model LSTM itu sendiri. Secara umum, script ini meliputi:\n",
    "<h4>Data Preprocessing</h4>\n",
    "- Padding & Truncating data, pada dasarnya memastikan seluruh trj_id memiliki jumlah data yang sama, yakni 20 data, dengan menambahkan data yang kurang dengan 0 dan memotong data yang kelebihan dan mengambil 20 data paling belakang.\n",
    "- Membagi data menjadi X (koordinat, keceptan, dll yang dimasukkan ke model untuk melatih model) dan y (koordinat yang benar untuk dibandingkan dengan koordinat hasil prediksi model). y akan diambil dari koordinat terakhir tiap trajectory.\n",
    "- Membagi data menjadi training data dan testing data, dengan pembagian 90%/10%.\n",
    "<br/>\n",
    "<h4>Model Training</h4>\n",
    "- Mendefinisikan dan melatih model LSTM berdasarkan data yang telah siap.\n",
    "- Arsitektur, epoch, loss, optimizer, dan dll dapat dengan bebas diubah-ubah untuk mencoba model lain.\n",
    "<br/>\n",
    "Jika ingin melihat performa model pertama yang telah saya latih, maka model itu sudah disave dalam folder models dengan nama model_v1.keras. Model dapat ditest menggunakan script Model Test.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Tensorboard\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          rawlat      rawlng      speed     bearing  hour_of_day  day_of_week\n",
      "trj_id                                                                       \n",
      "1      -6.198042  106.769008   4.322800  179.920000           14            3\n",
      "1      -6.200972  106.769202   8.014167  173.233333           14            3\n",
      "1      -6.205394  106.769768  10.116136  171.477273           14            3\n",
      "1      -6.210496  106.771217   9.307667  156.683333           14            3\n",
      "1      -6.214969  106.773830  10.103333  139.777778           14            3\n",
      "...          ...         ...        ...         ...          ...          ...\n",
      "9999   -6.187751  106.845707  10.584667  329.600000            4            6\n",
      "9999   -6.184123  106.843546   4.508780  324.512195            4            6\n",
      "9999   -6.182706  106.842869   2.776724  287.137931            4            6\n",
      "9999   -6.180504  106.842337   5.244333  326.850000            4            6\n",
      "9999   -6.179029  106.841998   2.330952  231.095238            4            6\n",
      "\n",
      "[1221233 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "## Read data from csv\n",
    "resampled_data = pd.read_csv('../clean_data.csv', index_col='trj_id').drop('Unnamed: 1', axis=1)\n",
    "print(resampled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad and truncate the timestamps in the dataframe\n",
    "# Ini buat kita samain input modelnya, gw potong timestampnya jadi pasti ada 20 timestamp per sample. Kalo lebih dipotong, kalo kurang ditambahin 0 di depannya\n",
    "# Pad value None --> pake koordinat pertama\n",
    "\n",
    "def pad_truncate_dataframe(df, max_len, padding='pre', truncating='post', pad_value=None):\n",
    "  # Split the dataframe by samples (first level of multi-index)\n",
    "  samples = df.groupby(level=0)\n",
    "\n",
    "  # Define a function to pad/truncate a single sample\n",
    "  def pad_truncate_sample(sample):\n",
    "    # Extract the values from a single sample\n",
    "    values = sample.values\n",
    "    first_element = values[0]\n",
    "    first_element[2] = 0\n",
    "    first_element[3] = 0\n",
    "\n",
    "    # Pad/truncate automatically using pad_sequences\n",
    "    if pad_value == None:\n",
    "      padded_truncated = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "          [values], maxlen=max_len, padding=padding, truncating=truncating, value=first_element, dtype='float64'\n",
    "      )[0]\n",
    "    else:\n",
    "      padded_truncated = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "          [values], maxlen=max_len, padding=padding, truncating=truncating, value=pad_value, dtype='float64'\n",
    "      )[0]\n",
    "\n",
    "    # Convert back to pandas dataframe\n",
    "    return pd.DataFrame(padded_truncated, columns=sample.columns)\n",
    "\n",
    "  # Apply the function to each sample and recreate the multi-index dataframe\n",
    "  padded_df = samples.apply(pad_truncate_sample)\n",
    "  return padded_df\n",
    "\n",
    "df_new = pad_truncate_dataframe(resampled_data, 21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55994, 21, 6)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Convert the multi index dataframe to a numpy 3D array for better integration to TensorFlow (samples, timesteps, features)\n",
    "numpy_data = df_new.to_xarray().to_array().to_numpy()\n",
    "numpy_data = np.transpose(numpy_data, (1, 2, 0))\n",
    "numpy_data.shape # Should be (55994, 21, 6) for 55994 samples, 21 timesteps, and 6 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Split the data to x (feature values) and y (target values)\n",
    "x_data = numpy_data[:, :-1, :]\n",
    "y_data = numpy_data[:, -1, 0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data to train and test splits (train to train the model, test to test the model on new data after trained)\n",
    "test_size = 0.1\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\adrie\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\layers\\preprocessing\\normalization.py:99: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "## Define normalization layer\n",
    "normalize_layer = tf.keras.layers.Normalization(axis=-1, input_shape=(20, 6))\n",
    "normalize_layer.adapt(numpy_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get Mean and Variance of Normalization layer to use for normalizing y and denormalizing later\n",
    "normalize_weights = normalize_layer.get_weights()\n",
    "\n",
    "mean_variance = np.array([normalize_weights[0][0:2], normalize_weights[1][0:2]])\n",
    "\n",
    "np.save('../mean_variance_A.npy', mean_variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 1.0495474  -0.8761074 ]\n",
      " [ 0.5334533   0.32078618]\n",
      " [ 0.03655405 -2.3028622 ]\n",
      " ...\n",
      " [ 0.6688767   2.004524  ]\n",
      " [-3.7857602  -0.38728026]\n",
      " [ 1.6493021  -1.7453458 ]], shape=(50394, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "## Normalize y\n",
    "normalize_y = tf.keras.layers.Normalization(mean=mean_variance[0], variance=mean_variance[1])\n",
    "\n",
    "y_train = normalize_y(y_train)\n",
    "y_test = normalize_y(y_test)\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save test data for model testing\n",
    "np.save('../x_test_A.npy', x_test)\n",
    "np.save('../y_test_A.npy', y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\adrie\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\backend\\common\\global_state.py:74: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ normalization (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Normalization</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)          │            <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">69,120</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │       <span style=\"color: #00af00; text-decoration-color: #00af00\">131,584</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">49,408</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">130</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ normalization (\u001b[38;5;33mNormalization\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m6\u001b[0m)          │            \u001b[38;5;34m13\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │        \u001b[38;5;34m69,120\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │       \u001b[38;5;34m131,584\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_2 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │        \u001b[38;5;34m49,408\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)              │           \u001b[38;5;34m130\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">250,255</span> (977.56 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m250,255\u001b[0m (977.56 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">250,242</span> (977.51 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m250,242\u001b[0m (977.51 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">13</span> (56.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m13\u001b[0m (56.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Clear any previous models\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# Name scope each layer\n",
    "\n",
    "# Define the model\n",
    "num_features = 6\n",
    "batch_size = 32\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(normalize_layer)\n",
    "model.add(tf.keras.layers.LSTM(128, return_sequences=True))\n",
    "model.add(tf.keras.layers.LSTM(128, return_sequences=True))\n",
    "model.add(tf.keras.layers.LSTM(64))\n",
    "model.add(tf.keras.layers.Dense(2))\n",
    "\n",
    "\n",
    "# Define optimizer\n",
    "lr = 1e-3\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "\n",
    "model.compile(loss='huber', optimizer=opt, metrics=['mae', 'mse'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define LR scheduling (optional if want to use or not)\n",
    "start_lr = lr\n",
    "min_lr = 0.00001\n",
    "max_lr = 0.001\n",
    "rampup_epochs = 0\n",
    "sustain_epochs = 0\n",
    "exp_decay = 0.0\n",
    "\n",
    "# Define the scheduling function\n",
    "def schedule(epoch):\n",
    "  def lr(epoch, start_lr, min_lr, max_lr, rampup_epochs, sustain_epochs, exp_decay):\n",
    "    if epoch < rampup_epochs:\n",
    "      lr = (max_lr - start_lr)/rampup_epochs * epoch + start_lr\n",
    "    elif epoch < rampup_epochs + sustain_epochs:\n",
    "      lr = max_lr\n",
    "    else:\n",
    "      lr = (max_lr - min_lr) * exp_decay**(epoch-rampup_epochs-sustain_epochs) + min_lr\n",
    "    return lr\n",
    "  return lr(epoch, start_lr, min_lr, max_lr, rampup_epochs, sustain_epochs, exp_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 57ms/step - loss: 0.0229 - mae: 0.1247 - mse: 0.0525 - val_loss: 0.0012 - val_mae: 0.0369 - val_mse: 0.0025\n",
      "Epoch 2/200\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 59ms/step - loss: 0.0011 - mae: 0.0345 - mse: 0.0022 - val_loss: 4.8963e-04 - val_mae: 0.0235 - val_mse: 9.7926e-04\n",
      "Epoch 3/200\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 57ms/step - loss: 5.4137e-04 - mae: 0.0244 - mse: 0.0011 - val_loss: 2.6161e-04 - val_mae: 0.0172 - val_mse: 5.2322e-04\n",
      "Epoch 4/200\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 58ms/step - loss: 3.4870e-04 - mae: 0.0194 - mse: 6.9740e-04 - val_loss: 5.0354e-04 - val_mae: 0.0243 - val_mse: 0.0010\n",
      "Epoch 5/200\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 57ms/step - loss: 3.0995e-04 - mae: 0.0182 - mse: 6.1990e-04 - val_loss: 5.4076e-04 - val_mae: 0.0222 - val_mse: 0.0011\n",
      "Epoch 6/200\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 59ms/step - loss: 2.8002e-04 - mae: 0.0167 - mse: 5.6004e-04 - val_loss: 1.7566e-04 - val_mae: 0.0143 - val_mse: 3.5132e-04\n",
      "Epoch 7/200\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 54ms/step - loss: 1.5957e-04 - mae: 0.0132 - mse: 3.1914e-04 - val_loss: 1.9113e-04 - val_mae: 0.0144 - val_mse: 3.8226e-04\n",
      "Epoch 8/200\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 54ms/step - loss: 2.3471e-04 - mae: 0.0153 - mse: 4.6942e-04 - val_loss: 1.5489e-04 - val_mae: 0.0132 - val_mse: 3.0978e-04\n",
      "Epoch 9/200\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 51ms/step - loss: 1.6932e-04 - mae: 0.0131 - mse: 3.3863e-04 - val_loss: 2.5724e-04 - val_mae: 0.0170 - val_mse: 5.1449e-04\n",
      "Epoch 10/200\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 53ms/step - loss: 1.6890e-04 - mae: 0.0132 - mse: 3.3780e-04 - val_loss: 0.0023 - val_mae: 0.0463 - val_mse: 0.0046\n",
      "Epoch 11/200\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 51ms/step - loss: 4.8376e-04 - mae: 0.0194 - mse: 9.6753e-04 - val_loss: 2.9157e-04 - val_mae: 0.0173 - val_mse: 5.8315e-04\n",
      "Epoch 12/200\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 49ms/step - loss: 1.5475e-04 - mae: 0.0126 - mse: 3.0951e-04 - val_loss: 1.6988e-04 - val_mae: 0.0136 - val_mse: 3.3977e-04\n",
      "Epoch 13/200\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 50ms/step - loss: 1.5301e-04 - mae: 0.0126 - mse: 3.0601e-04 - val_loss: 2.1227e-04 - val_mae: 0.0151 - val_mse: 4.2454e-04\n",
      "Epoch 14/200\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 52ms/step - loss: 2.1226e-04 - mae: 0.0143 - mse: 4.2451e-04 - val_loss: 9.7770e-05 - val_mae: 0.0102 - val_mse: 1.9554e-04\n",
      "Epoch 15/200\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 50ms/step - loss: 1.4538e-04 - mae: 0.0121 - mse: 2.9080e-04 - val_loss: 1.6434e-04 - val_mae: 0.0134 - val_mse: 3.2868e-04\n",
      "Epoch 16/200\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 50ms/step - loss: 1.4234e-04 - mae: 0.0120 - mse: 2.8469e-04 - val_loss: 1.2208e-04 - val_mae: 0.0114 - val_mse: 2.4416e-04\n",
      "Epoch 17/200\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 51ms/step - loss: 1.2876e-04 - mae: 0.0117 - mse: 2.5752e-04 - val_loss: 1.3671e-04 - val_mae: 0.0120 - val_mse: 2.7341e-04\n",
      "Epoch 18/200\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 51ms/step - loss: 1.3725e-04 - mae: 0.0120 - mse: 2.7450e-04 - val_loss: 2.2679e-04 - val_mae: 0.0152 - val_mse: 4.5358e-04\n",
      "Epoch 19/200\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 49ms/step - loss: 1.6698e-04 - mae: 0.0130 - mse: 3.3396e-04 - val_loss: 1.3716e-04 - val_mae: 0.0116 - val_mse: 2.7431e-04\n",
      "Epoch 20/200\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 53ms/step - loss: 1.3716e-04 - mae: 0.0120 - mse: 2.7431e-04 - val_loss: 8.6638e-05 - val_mae: 0.0096 - val_mse: 1.7328e-04\n",
      "Epoch 21/200\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 51ms/step - loss: 1.5107e-04 - mae: 0.0123 - mse: 3.0214e-04 - val_loss: 1.5643e-04 - val_mae: 0.0123 - val_mse: 3.1285e-04\n",
      "Epoch 22/200\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 51ms/step - loss: 1.3695e-04 - mae: 0.0119 - mse: 2.7390e-04 - val_loss: 1.0510e-04 - val_mae: 0.0108 - val_mse: 2.1020e-04\n",
      "Epoch 23/200\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 52ms/step - loss: 1.5158e-04 - mae: 0.0125 - mse: 3.0316e-04 - val_loss: 9.1913e-05 - val_mae: 0.0101 - val_mse: 1.8383e-04\n",
      "Epoch 24/200\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 51ms/step - loss: 1.2588e-04 - mae: 0.0115 - mse: 2.5176e-04 - val_loss: 1.7623e-04 - val_mae: 0.0139 - val_mse: 3.5246e-04\n",
      "Epoch 25/200\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 51ms/step - loss: 1.2827e-04 - mae: 0.0117 - mse: 2.5655e-04 - val_loss: 1.0572e-04 - val_mae: 0.0106 - val_mse: 2.1145e-04\n",
      "Epoch 26/200\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 51ms/step - loss: 1.2028e-04 - mae: 0.0113 - mse: 2.4055e-04 - val_loss: 1.9431e-04 - val_mae: 0.0144 - val_mse: 3.8862e-04\n",
      "Epoch 27/200\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 53ms/step - loss: 1.3517e-04 - mae: 0.0118 - mse: 2.7034e-04 - val_loss: 8.5399e-05 - val_mae: 0.0095 - val_mse: 1.7080e-04\n",
      "Epoch 28/200\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 49ms/step - loss: 1.1526e-04 - mae: 0.0111 - mse: 2.3052e-04 - val_loss: 9.7463e-05 - val_mae: 0.0102 - val_mse: 1.9493e-04\n",
      "Epoch 29/200\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 54ms/step - loss: 1.2448e-04 - mae: 0.0114 - mse: 2.4896e-04 - val_loss: 1.1357e-04 - val_mae: 0.0113 - val_mse: 2.2713e-04\n",
      "Epoch 30/200\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 49ms/step - loss: 1.1267e-04 - mae: 0.0110 - mse: 2.2534e-04 - val_loss: 1.1410e-04 - val_mae: 0.0110 - val_mse: 2.2820e-04\n",
      "Epoch 31/200\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 52ms/step - loss: 1.6125e-04 - mae: 0.0125 - mse: 3.2250e-04 - val_loss: 9.8546e-05 - val_mae: 0.0104 - val_mse: 1.9709e-04\n",
      "Epoch 32/200\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 51ms/step - loss: 1.0421e-04 - mae: 0.0105 - mse: 2.0842e-04 - val_loss: 1.1853e-04 - val_mae: 0.0114 - val_mse: 2.3705e-04\n",
      "Epoch 33/200\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 50ms/step - loss: 1.4651e-04 - mae: 0.0121 - mse: 2.9302e-04 - val_loss: 9.2478e-05 - val_mae: 0.0101 - val_mse: 1.8496e-04\n",
      "Epoch 34/200\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 48ms/step - loss: 1.1566e-04 - mae: 0.0111 - mse: 2.3133e-04 - val_loss: 1.0064e-04 - val_mae: 0.0105 - val_mse: 2.0127e-04\n",
      "Epoch 35/200\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 57ms/step - loss: 1.1896e-04 - mae: 0.0112 - mse: 2.3791e-04 - val_loss: 1.3914e-04 - val_mae: 0.0115 - val_mse: 2.7828e-04\n",
      "Epoch 36/200\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 71ms/step - loss: 1.6963e-04 - mae: 0.0124 - mse: 3.3925e-04 - val_loss: 9.9943e-05 - val_mae: 0.0105 - val_mse: 1.9989e-04\n",
      "Epoch 37/200\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 50ms/step - loss: 1.0881e-04 - mae: 0.0107 - mse: 2.1762e-04 - val_loss: 8.6069e-05 - val_mae: 0.0095 - val_mse: 1.7214e-04\n",
      "Epoch 38/200\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 49ms/step - loss: 1.1540e-04 - mae: 0.0110 - mse: 2.3079e-04 - val_loss: 9.8513e-05 - val_mae: 0.0101 - val_mse: 1.9703e-04\n",
      "Epoch 39/200\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 52ms/step - loss: 1.0671e-04 - mae: 0.0107 - mse: 2.1341e-04 - val_loss: 1.2669e-04 - val_mae: 0.0120 - val_mse: 2.5338e-04\n",
      "Epoch 40/200\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 54ms/step - loss: 1.1456e-04 - mae: 0.0110 - mse: 2.2912e-04 - val_loss: 1.0517e-04 - val_mae: 0.0108 - val_mse: 2.1034e-04\n",
      "Epoch 41/200\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 57ms/step - loss: 1.1741e-04 - mae: 0.0112 - mse: 2.3483e-04 - val_loss: 9.8028e-05 - val_mae: 0.0103 - val_mse: 1.9606e-04\n",
      "Epoch 42/200\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 54ms/step - loss: 1.1387e-04 - mae: 0.0110 - mse: 2.2775e-04 - val_loss: 8.8932e-05 - val_mae: 0.0099 - val_mse: 1.7786e-04\n",
      "Epoch 43/200\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 51ms/step - loss: 1.0635e-04 - mae: 0.0107 - mse: 2.1271e-04 - val_loss: 9.3187e-05 - val_mae: 0.0100 - val_mse: 1.8637e-04\n",
      "Epoch 44/200\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 46ms/step - loss: 1.2624e-04 - mae: 0.0114 - mse: 2.5247e-04 - val_loss: 8.3797e-05 - val_mae: 0.0095 - val_mse: 1.6759e-04\n",
      "Epoch 45/200\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 46ms/step - loss: 1.0514e-04 - mae: 0.0106 - mse: 2.1028e-04 - val_loss: 1.3046e-04 - val_mae: 0.0125 - val_mse: 2.6092e-04\n",
      "Epoch 46/200\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 45ms/step - loss: 1.0863e-04 - mae: 0.0107 - mse: 2.1726e-04 - val_loss: 9.1877e-05 - val_mae: 0.0099 - val_mse: 1.8375e-04\n",
      "Epoch 47/200\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 45ms/step - loss: 1.1492e-04 - mae: 0.0110 - mse: 2.2983e-04 - val_loss: 1.0555e-04 - val_mae: 0.0109 - val_mse: 2.1110e-04\n",
      "Epoch 48/200\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 46ms/step - loss: 1.0574e-04 - mae: 0.0107 - mse: 2.1148e-04 - val_loss: 9.9069e-05 - val_mae: 0.0102 - val_mse: 1.9814e-04\n",
      "Epoch 49/200\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 55ms/step - loss: 1.5857e-04 - mae: 0.0119 - mse: 3.1714e-04 - val_loss: 1.2672e-04 - val_mae: 0.0122 - val_mse: 2.5343e-04\n",
      "Epoch 50/200\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 53ms/step - loss: 9.9348e-05 - mae: 0.0103 - mse: 1.9870e-04 - val_loss: 1.1499e-04 - val_mae: 0.0112 - val_mse: 2.2997e-04\n",
      "Epoch 51/200\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 49ms/step - loss: 1.0112e-04 - mae: 0.0104 - mse: 2.0225e-04 - val_loss: 9.8647e-05 - val_mae: 0.0104 - val_mse: 1.9729e-04\n",
      "Epoch 52/200\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 58ms/step - loss: 1.0907e-04 - mae: 0.0107 - mse: 2.1813e-04 - val_loss: 9.7154e-05 - val_mae: 0.0104 - val_mse: 1.9431e-04\n",
      "Epoch 53/200\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 59ms/step - loss: 1.0479e-04 - mae: 0.0105 - mse: 2.0959e-04 - val_loss: 9.4065e-05 - val_mae: 0.0102 - val_mse: 1.8813e-04\n",
      "Epoch 54/200\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 50ms/step - loss: 1.0808e-04 - mae: 0.0106 - mse: 2.1616e-04 - val_loss: 8.6713e-05 - val_mae: 0.0097 - val_mse: 1.7343e-04\n",
      "Epoch 55/200\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 57ms/step - loss: 1.2411e-04 - mae: 0.0112 - mse: 2.4822e-04 - val_loss: 9.7310e-05 - val_mae: 0.0103 - val_mse: 1.9462e-04\n",
      "Epoch 56/200\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 49ms/step - loss: 1.0218e-04 - mae: 0.0105 - mse: 2.0437e-04 - val_loss: 8.9826e-05 - val_mae: 0.0095 - val_mse: 1.7965e-04\n",
      "Epoch 57/200\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 55ms/step - loss: 1.0038e-04 - mae: 0.0104 - mse: 2.0075e-04 - val_loss: 8.7541e-05 - val_mae: 0.0098 - val_mse: 1.7508e-04\n",
      "Epoch 58/200\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 53ms/step - loss: 1.1704e-04 - mae: 0.0110 - mse: 2.3407e-04 - val_loss: 9.3749e-05 - val_mae: 0.0102 - val_mse: 1.8750e-04\n",
      "Epoch 59/200\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 51ms/step - loss: 1.0210e-04 - mae: 0.0104 - mse: 2.0420e-04 - val_loss: 1.1585e-04 - val_mae: 0.0110 - val_mse: 2.3170e-04\n",
      "Epoch 60/200\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 51ms/step - loss: 1.0828e-04 - mae: 0.0106 - mse: 2.1657e-04 - val_loss: 1.1939e-04 - val_mae: 0.0116 - val_mse: 2.3878e-04\n",
      "Epoch 61/200\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 49ms/step - loss: 1.0098e-04 - mae: 0.0104 - mse: 2.0196e-04 - val_loss: 9.3975e-05 - val_mae: 0.0101 - val_mse: 1.8795e-04\n",
      "Epoch 62/200\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 50ms/step - loss: 9.8354e-05 - mae: 0.0102 - mse: 1.9671e-04 - val_loss: 9.8420e-05 - val_mae: 0.0103 - val_mse: 1.9684e-04\n",
      "Epoch 63/200\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 58ms/step - loss: 1.0337e-04 - mae: 0.0105 - mse: 2.0674e-04 - val_loss: 1.1368e-04 - val_mae: 0.0107 - val_mse: 2.2735e-04\n",
      "Epoch 64/200\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 47ms/step - loss: 1.0001e-04 - mae: 0.0102 - mse: 2.0002e-04 - val_loss: 9.7357e-05 - val_mae: 0.0099 - val_mse: 1.9471e-04\n",
      "Epoch 65/200\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 49ms/step - loss: 1.0457e-04 - mae: 0.0104 - mse: 2.0914e-04 - val_loss: 8.2029e-05 - val_mae: 0.0094 - val_mse: 1.6406e-04\n",
      "Epoch 66/200\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 49ms/step - loss: 9.4018e-05 - mae: 0.0100 - mse: 1.8804e-04 - val_loss: 1.1365e-04 - val_mae: 0.0114 - val_mse: 2.2730e-04\n",
      "Epoch 67/200\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 47ms/step - loss: 1.0617e-04 - mae: 0.0106 - mse: 2.1234e-04 - val_loss: 9.1638e-05 - val_mae: 0.0100 - val_mse: 1.8328e-04\n",
      "Epoch 68/200\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 62ms/step - loss: 1.0055e-04 - mae: 0.0102 - mse: 2.0110e-04 - val_loss: 8.8595e-05 - val_mae: 0.0097 - val_mse: 1.7719e-04\n",
      "Epoch 69/200\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 48ms/step - loss: 9.8895e-05 - mae: 0.0102 - mse: 1.9779e-04 - val_loss: 8.0949e-05 - val_mae: 0.0093 - val_mse: 1.6190e-04\n",
      "Epoch 70/200\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 47ms/step - loss: 1.0851e-04 - mae: 0.0106 - mse: 2.1703e-04 - val_loss: 7.7561e-05 - val_mae: 0.0090 - val_mse: 1.5512e-04\n",
      "Epoch 71/200\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 47ms/step - loss: 9.3526e-05 - mae: 0.0100 - mse: 1.8705e-04 - val_loss: 1.2857e-04 - val_mae: 0.0116 - val_mse: 2.5713e-04\n",
      "Epoch 72/200\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 46ms/step - loss: 9.1692e-05 - mae: 0.0099 - mse: 1.8338e-04 - val_loss: 1.0668e-04 - val_mae: 0.0109 - val_mse: 2.1337e-04\n",
      "Epoch 73/200\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 46ms/step - loss: 1.0330e-04 - mae: 0.0104 - mse: 2.0661e-04 - val_loss: 1.0290e-04 - val_mae: 0.0105 - val_mse: 2.0580e-04\n",
      "Epoch 74/200\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 53ms/step - loss: 1.0882e-04 - mae: 0.0106 - mse: 2.1764e-04 - val_loss: 7.5904e-05 - val_mae: 0.0089 - val_mse: 1.5181e-04\n",
      "Epoch 75/200\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 45ms/step - loss: 9.4761e-05 - mae: 0.0101 - mse: 1.8952e-04 - val_loss: 7.9864e-05 - val_mae: 0.0092 - val_mse: 1.5973e-04\n",
      "Epoch 76/200\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 52ms/step - loss: 9.9343e-05 - mae: 0.0102 - mse: 1.9869e-04 - val_loss: 1.5175e-04 - val_mae: 0.0130 - val_mse: 3.0349e-04\n",
      "Epoch 77/200\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 47ms/step - loss: 1.0042e-04 - mae: 0.0103 - mse: 2.0084e-04 - val_loss: 1.0376e-04 - val_mae: 0.0106 - val_mse: 2.0752e-04\n",
      "Epoch 78/200\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 51ms/step - loss: 1.0662e-04 - mae: 0.0105 - mse: 2.1324e-04 - val_loss: 1.0666e-04 - val_mae: 0.0102 - val_mse: 2.1332e-04\n",
      "Epoch 79/200\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 49ms/step - loss: 9.3601e-05 - mae: 0.0100 - mse: 1.8720e-04 - val_loss: 1.4747e-04 - val_mae: 0.0125 - val_mse: 2.9494e-04\n",
      "Epoch 80/200\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 47ms/step - loss: 1.0105e-04 - mae: 0.0104 - mse: 2.0210e-04 - val_loss: 8.1570e-05 - val_mae: 0.0093 - val_mse: 1.6314e-04\n",
      "Epoch 81/200\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 54ms/step - loss: 9.1022e-05 - mae: 0.0099 - mse: 1.8204e-04 - val_loss: 8.8519e-05 - val_mae: 0.0097 - val_mse: 1.7704e-04\n",
      "Epoch 82/200\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 58ms/step - loss: 8.5756e-05 - mae: 0.0096 - mse: 1.7151e-04 - val_loss: 1.8306e-04 - val_mae: 0.0133 - val_mse: 3.6612e-04\n",
      "Epoch 83/200\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 56ms/step - loss: 1.1731e-04 - mae: 0.0108 - mse: 2.3462e-04 - val_loss: 8.2118e-05 - val_mae: 0.0092 - val_mse: 1.6424e-04\n",
      "Epoch 84/200\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 51ms/step - loss: 9.5719e-05 - mae: 0.0101 - mse: 1.9144e-04 - val_loss: 7.8587e-05 - val_mae: 0.0092 - val_mse: 1.5717e-04\n",
      "Epoch 85/200\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 48ms/step - loss: 9.8241e-05 - mae: 0.0102 - mse: 1.9648e-04 - val_loss: 1.0537e-04 - val_mae: 0.0108 - val_mse: 2.1074e-04\n",
      "Epoch 86/200\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 47ms/step - loss: 9.0751e-05 - mae: 0.0097 - mse: 1.8150e-04 - val_loss: 1.0595e-04 - val_mae: 0.0108 - val_mse: 2.1189e-04\n",
      "Epoch 87/200\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 46ms/step - loss: 9.9636e-05 - mae: 0.0103 - mse: 1.9927e-04 - val_loss: 8.0050e-05 - val_mae: 0.0092 - val_mse: 1.6010e-04\n",
      "Epoch 88/200\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 47ms/step - loss: 8.9585e-05 - mae: 0.0098 - mse: 1.7917e-04 - val_loss: 1.0986e-04 - val_mae: 0.0111 - val_mse: 2.1973e-04\n",
      "Epoch 89/200\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 46ms/step - loss: 9.2020e-05 - mae: 0.0099 - mse: 1.8404e-04 - val_loss: 8.8236e-05 - val_mae: 0.0098 - val_mse: 1.7647e-04\n",
      "Epoch 90/200\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 47ms/step - loss: 8.8810e-05 - mae: 0.0098 - mse: 1.7762e-04 - val_loss: 8.5121e-05 - val_mae: 0.0095 - val_mse: 1.7024e-04\n",
      "Epoch 91/200\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 45ms/step - loss: 8.7767e-05 - mae: 0.0097 - mse: 1.7553e-04 - val_loss: 8.3769e-05 - val_mae: 0.0093 - val_mse: 1.6754e-04\n",
      "Epoch 92/200\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 47ms/step - loss: 9.4080e-05 - mae: 0.0100 - mse: 1.8816e-04 - val_loss: 8.2527e-05 - val_mae: 0.0094 - val_mse: 1.6505e-04\n",
      "Epoch 93/200\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 46ms/step - loss: 9.9771e-05 - mae: 0.0103 - mse: 1.9954e-04 - val_loss: 8.7602e-05 - val_mae: 0.0098 - val_mse: 1.7520e-04\n",
      "Epoch 94/200\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 46ms/step - loss: 1.0102e-04 - mae: 0.0103 - mse: 2.0205e-04 - val_loss: 7.0374e-05 - val_mae: 0.0085 - val_mse: 1.4075e-04\n",
      "Epoch 95/200\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 46ms/step - loss: 8.6827e-05 - mae: 0.0096 - mse: 1.7365e-04 - val_loss: 8.3419e-05 - val_mae: 0.0094 - val_mse: 1.6684e-04\n",
      "Epoch 96/200\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 48ms/step - loss: 9.1456e-05 - mae: 0.0098 - mse: 1.8291e-04 - val_loss: 1.0658e-04 - val_mae: 0.0106 - val_mse: 2.1315e-04\n",
      "Epoch 97/200\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 47ms/step - loss: 9.8566e-05 - mae: 0.0102 - mse: 1.9713e-04 - val_loss: 9.8791e-05 - val_mae: 0.0104 - val_mse: 1.9758e-04\n",
      "Epoch 98/200\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 54ms/step - loss: 8.8712e-05 - mae: 0.0097 - mse: 1.7742e-04 - val_loss: 2.7784e-04 - val_mae: 0.0175 - val_mse: 5.5568e-04\n",
      "Epoch 99/200\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 55ms/step - loss: 2.1313e-04 - mae: 0.0127 - mse: 4.2626e-04 - val_loss: 9.4292e-05 - val_mae: 0.0098 - val_mse: 1.8858e-04\n",
      "Epoch 100/200\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 50ms/step - loss: 8.4989e-05 - mae: 0.0096 - mse: 1.6998e-04 - val_loss: 8.8221e-05 - val_mae: 0.0099 - val_mse: 1.7644e-04\n",
      "Epoch 101/200\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 49ms/step - loss: 8.4842e-05 - mae: 0.0095 - mse: 1.6968e-04 - val_loss: 9.3752e-05 - val_mae: 0.0100 - val_mse: 1.8750e-04\n",
      "Epoch 102/200\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 52ms/step - loss: 8.9801e-05 - mae: 0.0098 - mse: 1.7960e-04 - val_loss: 8.0463e-05 - val_mae: 0.0093 - val_mse: 1.6093e-04\n",
      "Epoch 103/200\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 51ms/step - loss: 8.8485e-05 - mae: 0.0097 - mse: 1.7697e-04 - val_loss: 9.8388e-05 - val_mae: 0.0104 - val_mse: 1.9678e-04\n",
      "Epoch 104/200\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 51ms/step - loss: 8.7218e-05 - mae: 0.0097 - mse: 1.7444e-04 - val_loss: 8.3786e-05 - val_mae: 0.0094 - val_mse: 1.6757e-04\n",
      "Epoch 105/200\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 54ms/step - loss: 8.7041e-05 - mae: 0.0096 - mse: 1.7408e-04 - val_loss: 1.0978e-04 - val_mae: 0.0111 - val_mse: 2.1957e-04\n",
      "Epoch 106/200\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 58ms/step - loss: 8.5514e-05 - mae: 0.0095 - mse: 1.7103e-04 - val_loss: 1.2058e-04 - val_mae: 0.0116 - val_mse: 2.4116e-04\n",
      "Epoch 107/200\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 53ms/step - loss: 8.7321e-05 - mae: 0.0097 - mse: 1.7464e-04 - val_loss: 9.7128e-05 - val_mae: 0.0105 - val_mse: 1.9426e-04\n",
      "Epoch 108/200\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 54ms/step - loss: 8.4891e-05 - mae: 0.0095 - mse: 1.6978e-04 - val_loss: 1.0055e-04 - val_mae: 0.0105 - val_mse: 2.0110e-04\n",
      "Epoch 109/200\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 52ms/step - loss: 8.7223e-05 - mae: 0.0096 - mse: 1.7445e-04 - val_loss: 7.3062e-05 - val_mae: 0.0086 - val_mse: 1.4612e-04\n",
      "Epoch 110/200\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 55ms/step - loss: 8.5637e-05 - mae: 0.0095 - mse: 1.7127e-04 - val_loss: 8.9194e-05 - val_mae: 0.0100 - val_mse: 1.7839e-04\n",
      "Epoch 111/200\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 54ms/step - loss: 8.6011e-05 - mae: 0.0095 - mse: 1.7202e-04 - val_loss: 8.5544e-05 - val_mae: 0.0095 - val_mse: 1.7109e-04\n",
      "Epoch 112/200\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 48ms/step - loss: 8.2795e-05 - mae: 0.0094 - mse: 1.6559e-04 - val_loss: 1.0979e-04 - val_mae: 0.0102 - val_mse: 2.1959e-04\n",
      "Epoch 113/200\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 42ms/step - loss: 8.3213e-05 - mae: 0.0094 - mse: 1.6643e-04 - val_loss: 7.8842e-05 - val_mae: 0.0091 - val_mse: 1.5768e-04\n",
      "Epoch 114/200\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 42ms/step - loss: 8.2361e-05 - mae: 0.0094 - mse: 1.6472e-04 - val_loss: 8.0759e-05 - val_mae: 0.0094 - val_mse: 1.6152e-04\n",
      "Epoch 115/200\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 42ms/step - loss: 8.1744e-05 - mae: 0.0093 - mse: 1.6349e-04 - val_loss: 1.0322e-04 - val_mae: 0.0106 - val_mse: 2.0644e-04\n",
      "Epoch 116/200\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 42ms/step - loss: 8.8246e-05 - mae: 0.0097 - mse: 1.7649e-04 - val_loss: 8.0275e-05 - val_mae: 0.0093 - val_mse: 1.6055e-04\n",
      "Epoch 117/200\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 42ms/step - loss: 8.5445e-05 - mae: 0.0095 - mse: 1.7089e-04 - val_loss: 9.3347e-05 - val_mae: 0.0100 - val_mse: 1.8669e-04\n",
      "Epoch 118/200\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 45ms/step - loss: 8.9660e-05 - mae: 0.0098 - mse: 1.7932e-04 - val_loss: 8.5595e-05 - val_mae: 0.0095 - val_mse: 1.7119e-04\n",
      "Epoch 119/200\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 43ms/step - loss: 8.4339e-05 - mae: 0.0095 - mse: 1.6868e-04 - val_loss: 8.0369e-05 - val_mae: 0.0091 - val_mse: 1.6074e-04\n",
      "Epoch 120/200\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 42ms/step - loss: 8.4365e-05 - mae: 0.0094 - mse: 1.6873e-04 - val_loss: 8.2355e-05 - val_mae: 0.0092 - val_mse: 1.6471e-04\n",
      "Epoch 121/200\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 43ms/step - loss: 8.9731e-05 - mae: 0.0098 - mse: 1.7946e-04 - val_loss: 7.7411e-05 - val_mae: 0.0091 - val_mse: 1.5482e-04\n",
      "Epoch 122/200\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 43ms/step - loss: 8.1420e-05 - mae: 0.0093 - mse: 1.6284e-04 - val_loss: 1.0097e-04 - val_mae: 0.0105 - val_mse: 2.0194e-04\n",
      "Epoch 123/200\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 43ms/step - loss: 8.4942e-05 - mae: 0.0095 - mse: 1.6988e-04 - val_loss: 7.9674e-05 - val_mae: 0.0092 - val_mse: 1.5935e-04\n",
      "Epoch 124/200\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 42ms/step - loss: 8.3743e-05 - mae: 0.0095 - mse: 1.6749e-04 - val_loss: 7.9779e-05 - val_mae: 0.0092 - val_mse: 1.5956e-04\n",
      "Epoch 125/200\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 42ms/step - loss: 8.6411e-05 - mae: 0.0096 - mse: 1.7282e-04 - val_loss: 8.5432e-05 - val_mae: 0.0097 - val_mse: 1.7086e-04\n",
      "Epoch 126/200\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 43ms/step - loss: 8.8944e-05 - mae: 0.0097 - mse: 1.7789e-04 - val_loss: 8.5723e-05 - val_mae: 0.0096 - val_mse: 1.7145e-04\n",
      "Epoch 127/200\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 43ms/step - loss: 8.1743e-05 - mae: 0.0094 - mse: 1.6349e-04 - val_loss: 9.2949e-05 - val_mae: 0.0100 - val_mse: 1.8590e-04\n",
      "Epoch 128/200\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 43ms/step - loss: 8.2792e-05 - mae: 0.0094 - mse: 1.6558e-04 - val_loss: 8.5256e-05 - val_mae: 0.0096 - val_mse: 1.7051e-04\n",
      "Epoch 129/200\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 42ms/step - loss: 8.1306e-05 - mae: 0.0093 - mse: 1.6261e-04 - val_loss: 9.1605e-05 - val_mae: 0.0100 - val_mse: 1.8321e-04\n",
      "Epoch 130/200\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 43ms/step - loss: 8.8520e-05 - mae: 0.0097 - mse: 1.7704e-04 - val_loss: 7.6918e-05 - val_mae: 0.0089 - val_mse: 1.5384e-04\n",
      "Epoch 131/200\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 43ms/step - loss: 8.2174e-05 - mae: 0.0093 - mse: 1.6435e-04 - val_loss: 8.3174e-05 - val_mae: 0.0094 - val_mse: 1.6635e-04\n",
      "Epoch 132/200\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 43ms/step - loss: 8.6249e-05 - mae: 0.0096 - mse: 1.7250e-04 - val_loss: 7.9319e-05 - val_mae: 0.0091 - val_mse: 1.5864e-04\n",
      "Epoch 133/200\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 43ms/step - loss: 7.6613e-05 - mae: 0.0091 - mse: 1.5323e-04 - val_loss: 1.0132e-04 - val_mae: 0.0100 - val_mse: 2.0264e-04\n",
      "Epoch 134/200\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 43ms/step - loss: 8.7812e-05 - mae: 0.0097 - mse: 1.7562e-04 - val_loss: 1.3471e-04 - val_mae: 0.0119 - val_mse: 2.6941e-04\n",
      "Epoch 135/200\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 43ms/step - loss: 8.4064e-05 - mae: 0.0095 - mse: 1.6813e-04 - val_loss: 9.9092e-05 - val_mae: 0.0102 - val_mse: 1.9818e-04\n",
      "Epoch 136/200\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 44ms/step - loss: 7.8751e-05 - mae: 0.0092 - mse: 1.5750e-04 - val_loss: 9.0625e-05 - val_mae: 0.0098 - val_mse: 1.8125e-04\n",
      "Epoch 137/200\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 43ms/step - loss: 8.3361e-05 - mae: 0.0095 - mse: 1.6672e-04 - val_loss: 1.6392e-04 - val_mae: 0.0131 - val_mse: 3.2784e-04\n",
      "Epoch 138/200\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 43ms/step - loss: 9.1966e-05 - mae: 0.0098 - mse: 1.8393e-04 - val_loss: 8.2388e-05 - val_mae: 0.0093 - val_mse: 1.6478e-04\n",
      "Epoch 139/200\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 43ms/step - loss: 7.9419e-05 - mae: 0.0093 - mse: 1.5884e-04 - val_loss: 9.8849e-05 - val_mae: 0.0106 - val_mse: 1.9770e-04\n",
      "Epoch 140/200\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 43ms/step - loss: 8.0760e-05 - mae: 0.0094 - mse: 1.6152e-04 - val_loss: 1.6565e-04 - val_mae: 0.0139 - val_mse: 3.3131e-04\n",
      "Epoch 141/200\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 44ms/step - loss: 8.8107e-05 - mae: 0.0097 - mse: 1.7621e-04 - val_loss: 8.1671e-05 - val_mae: 0.0093 - val_mse: 1.6334e-04\n",
      "Epoch 142/200\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 43ms/step - loss: 7.8508e-05 - mae: 0.0092 - mse: 1.5702e-04 - val_loss: 7.6182e-05 - val_mae: 0.0088 - val_mse: 1.5236e-04\n",
      "Epoch 143/200\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 43ms/step - loss: 8.3474e-05 - mae: 0.0094 - mse: 1.6695e-04 - val_loss: 1.0599e-04 - val_mae: 0.0109 - val_mse: 2.1198e-04\n",
      "Epoch 144/200\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 43ms/step - loss: 7.9602e-05 - mae: 0.0092 - mse: 1.5920e-04 - val_loss: 8.7869e-05 - val_mae: 0.0099 - val_mse: 1.7574e-04\n",
      "Epoch 145/200\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 44ms/step - loss: 8.6377e-05 - mae: 0.0096 - mse: 1.7275e-04 - val_loss: 8.3691e-05 - val_mae: 0.0092 - val_mse: 1.6738e-04\n",
      "Epoch 146/200\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 44ms/step - loss: 7.6631e-05 - mae: 0.0091 - mse: 1.5326e-04 - val_loss: 7.8745e-05 - val_mae: 0.0091 - val_mse: 1.5749e-04\n",
      "Epoch 147/200\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 44ms/step - loss: 8.0169e-05 - mae: 0.0092 - mse: 1.6034e-04 - val_loss: 1.6925e-04 - val_mae: 0.0130 - val_mse: 3.3850e-04\n",
      "Epoch 148/200\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 43ms/step - loss: 8.4356e-05 - mae: 0.0095 - mse: 1.6871e-04 - val_loss: 8.7659e-05 - val_mae: 0.0098 - val_mse: 1.7532e-04\n",
      "Epoch 149/200\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 43ms/step - loss: 7.7402e-05 - mae: 0.0091 - mse: 1.5480e-04 - val_loss: 7.9228e-05 - val_mae: 0.0091 - val_mse: 1.5846e-04\n",
      "Epoch 150/200\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 44ms/step - loss: 8.0823e-05 - mae: 0.0094 - mse: 1.6165e-04 - val_loss: 8.2815e-05 - val_mae: 0.0090 - val_mse: 1.6563e-04\n",
      "Epoch 151/200\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 44ms/step - loss: 8.0037e-05 - mae: 0.0093 - mse: 1.6007e-04 - val_loss: 8.3715e-05 - val_mae: 0.0092 - val_mse: 1.6743e-04\n",
      "Epoch 152/200\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 44ms/step - loss: 8.3482e-05 - mae: 0.0094 - mse: 1.6696e-04 - val_loss: 1.0377e-04 - val_mae: 0.0106 - val_mse: 2.0755e-04\n",
      "Epoch 153/200\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 44ms/step - loss: 7.6731e-05 - mae: 0.0091 - mse: 1.5346e-04 - val_loss: 9.2304e-05 - val_mae: 0.0100 - val_mse: 1.8461e-04\n",
      "Epoch 154/200\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 44ms/step - loss: 8.3659e-05 - mae: 0.0094 - mse: 1.6732e-04 - val_loss: 7.4946e-05 - val_mae: 0.0088 - val_mse: 1.4989e-04\n",
      "Epoch 155/200\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 44ms/step - loss: 7.4756e-05 - mae: 0.0089 - mse: 1.4951e-04 - val_loss: 7.6210e-05 - val_mae: 0.0089 - val_mse: 1.5242e-04\n",
      "Epoch 156/200\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 44ms/step - loss: 7.4199e-05 - mae: 0.0089 - mse: 1.4840e-04 - val_loss: 1.0042e-04 - val_mae: 0.0102 - val_mse: 2.0085e-04\n",
      "Epoch 157/200\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 44ms/step - loss: 7.6850e-05 - mae: 0.0091 - mse: 1.5370e-04 - val_loss: 8.2455e-05 - val_mae: 0.0094 - val_mse: 1.6491e-04\n",
      "Epoch 158/200\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 44ms/step - loss: 7.7768e-05 - mae: 0.0091 - mse: 1.5554e-04 - val_loss: 8.1834e-05 - val_mae: 0.0093 - val_mse: 1.6367e-04\n",
      "Epoch 159/200\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 45ms/step - loss: 7.5184e-05 - mae: 0.0090 - mse: 1.5037e-04 - val_loss: 9.7864e-05 - val_mae: 0.0104 - val_mse: 1.9573e-04\n",
      "Epoch 160/200\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 44ms/step - loss: 8.4154e-05 - mae: 0.0095 - mse: 1.6831e-04 - val_loss: 9.3031e-05 - val_mae: 0.0101 - val_mse: 1.8606e-04\n",
      "Epoch 161/200\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 44ms/step - loss: 7.6255e-05 - mae: 0.0090 - mse: 1.5251e-04 - val_loss: 7.8951e-05 - val_mae: 0.0092 - val_mse: 1.5790e-04\n",
      "Epoch 162/200\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 44ms/step - loss: 7.4960e-05 - mae: 0.0090 - mse: 1.4992e-04 - val_loss: 8.3279e-05 - val_mae: 0.0094 - val_mse: 1.6656e-04\n",
      "Epoch 163/200\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 44ms/step - loss: 8.3652e-05 - mae: 0.0095 - mse: 1.6730e-04 - val_loss: 8.0876e-05 - val_mae: 0.0092 - val_mse: 1.6175e-04\n",
      "Epoch 164/200\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 44ms/step - loss: 9.1083e-05 - mae: 0.0097 - mse: 1.8217e-04 - val_loss: 8.4640e-05 - val_mae: 0.0095 - val_mse: 1.6928e-04\n",
      "Epoch 165/200\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 44ms/step - loss: 7.4157e-05 - mae: 0.0090 - mse: 1.4831e-04 - val_loss: 8.6638e-05 - val_mae: 0.0097 - val_mse: 1.7328e-04\n",
      "Epoch 166/200\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 45ms/step - loss: 7.6818e-05 - mae: 0.0091 - mse: 1.5364e-04 - val_loss: 8.7792e-05 - val_mae: 0.0097 - val_mse: 1.7558e-04\n",
      "Epoch 167/200\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 44ms/step - loss: 7.5507e-05 - mae: 0.0090 - mse: 1.5101e-04 - val_loss: 7.9028e-05 - val_mae: 0.0090 - val_mse: 1.5806e-04\n",
      "Epoch 168/200\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 45ms/step - loss: 7.3799e-05 - mae: 0.0090 - mse: 1.4760e-04 - val_loss: 7.4473e-05 - val_mae: 0.0087 - val_mse: 1.4895e-04\n",
      "Epoch 169/200\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 45ms/step - loss: 7.3903e-05 - mae: 0.0089 - mse: 1.4781e-04 - val_loss: 8.1637e-05 - val_mae: 0.0092 - val_mse: 1.6327e-04\n",
      "Epoch 170/200\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 44ms/step - loss: 7.3852e-05 - mae: 0.0090 - mse: 1.4770e-04 - val_loss: 7.9435e-05 - val_mae: 0.0092 - val_mse: 1.5887e-04\n",
      "Epoch 171/200\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 44ms/step - loss: 7.4299e-05 - mae: 0.0090 - mse: 1.4860e-04 - val_loss: 7.9284e-05 - val_mae: 0.0092 - val_mse: 1.5857e-04\n",
      "Epoch 172/200\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 44ms/step - loss: 7.6529e-05 - mae: 0.0091 - mse: 1.5306e-04 - val_loss: 1.1843e-04 - val_mae: 0.0108 - val_mse: 2.3685e-04\n",
      "Epoch 173/200\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 45ms/step - loss: 7.9502e-05 - mae: 0.0092 - mse: 1.5900e-04 - val_loss: 8.0085e-05 - val_mae: 0.0091 - val_mse: 1.6017e-04\n",
      "Epoch 174/200\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 44ms/step - loss: 7.7931e-05 - mae: 0.0091 - mse: 1.5586e-04 - val_loss: 8.0531e-05 - val_mae: 0.0093 - val_mse: 1.6106e-04\n",
      "Epoch 175/200\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 45ms/step - loss: 7.6733e-05 - mae: 0.0091 - mse: 1.5347e-04 - val_loss: 7.2003e-05 - val_mae: 0.0085 - val_mse: 1.4401e-04\n",
      "Epoch 176/200\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 44ms/step - loss: 7.8372e-05 - mae: 0.0091 - mse: 1.5674e-04 - val_loss: 7.9496e-05 - val_mae: 0.0092 - val_mse: 1.5899e-04\n",
      "Epoch 177/200\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 45ms/step - loss: 7.2491e-05 - mae: 0.0089 - mse: 1.4498e-04 - val_loss: 8.5760e-05 - val_mae: 0.0095 - val_mse: 1.7152e-04\n",
      "Epoch 178/200\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 44ms/step - loss: 8.2123e-05 - mae: 0.0094 - mse: 1.6425e-04 - val_loss: 7.7874e-05 - val_mae: 0.0090 - val_mse: 1.5575e-04\n",
      "Epoch 179/200\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 44ms/step - loss: 7.4369e-05 - mae: 0.0089 - mse: 1.4874e-04 - val_loss: 7.9348e-05 - val_mae: 0.0091 - val_mse: 1.5870e-04\n",
      "Epoch 180/200\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 44ms/step - loss: 7.4942e-05 - mae: 0.0090 - mse: 1.4988e-04 - val_loss: 7.5827e-05 - val_mae: 0.0089 - val_mse: 1.5165e-04\n",
      "Epoch 181/200\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 44ms/step - loss: 7.2497e-05 - mae: 0.0089 - mse: 1.4499e-04 - val_loss: 7.8269e-05 - val_mae: 0.0090 - val_mse: 1.5654e-04\n",
      "Epoch 182/200\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 45ms/step - loss: 7.2066e-05 - mae: 0.0088 - mse: 1.4413e-04 - val_loss: 9.7629e-05 - val_mae: 0.0101 - val_mse: 1.9526e-04\n",
      "Epoch 183/200\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 45ms/step - loss: 7.6780e-05 - mae: 0.0091 - mse: 1.5356e-04 - val_loss: 8.7133e-05 - val_mae: 0.0094 - val_mse: 1.7427e-04\n",
      "Epoch 184/200\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 44ms/step - loss: 7.7865e-05 - mae: 0.0091 - mse: 1.5573e-04 - val_loss: 1.3478e-04 - val_mae: 0.0124 - val_mse: 2.6957e-04\n",
      "Epoch 185/200\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 45ms/step - loss: 7.3905e-05 - mae: 0.0089 - mse: 1.4781e-04 - val_loss: 9.5713e-05 - val_mae: 0.0098 - val_mse: 1.9143e-04\n",
      "Epoch 186/200\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 45ms/step - loss: 8.4147e-05 - mae: 0.0093 - mse: 1.6829e-04 - val_loss: 7.4763e-05 - val_mae: 0.0088 - val_mse: 1.4953e-04\n",
      "Epoch 187/200\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 45ms/step - loss: 7.1245e-05 - mae: 0.0088 - mse: 1.4249e-04 - val_loss: 7.3468e-05 - val_mae: 0.0087 - val_mse: 1.4694e-04\n",
      "Epoch 188/200\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 45ms/step - loss: 7.1447e-05 - mae: 0.0088 - mse: 1.4289e-04 - val_loss: 7.5850e-05 - val_mae: 0.0088 - val_mse: 1.5170e-04\n",
      "Epoch 189/200\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 45ms/step - loss: 6.9518e-05 - mae: 0.0087 - mse: 1.3904e-04 - val_loss: 8.0515e-05 - val_mae: 0.0092 - val_mse: 1.6103e-04\n",
      "Epoch 190/200\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 45ms/step - loss: 7.4236e-05 - mae: 0.0090 - mse: 1.4847e-04 - val_loss: 7.8368e-05 - val_mae: 0.0090 - val_mse: 1.5674e-04\n",
      "Epoch 191/200\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 45ms/step - loss: 7.5881e-05 - mae: 0.0090 - mse: 1.5176e-04 - val_loss: 7.7519e-05 - val_mae: 0.0090 - val_mse: 1.5504e-04\n",
      "Epoch 192/200\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 45ms/step - loss: 7.2113e-05 - mae: 0.0088 - mse: 1.4423e-04 - val_loss: 8.0996e-05 - val_mae: 0.0093 - val_mse: 1.6199e-04\n",
      "Epoch 193/200\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 46ms/step - loss: 7.1558e-05 - mae: 0.0088 - mse: 1.4312e-04 - val_loss: 7.8238e-05 - val_mae: 0.0090 - val_mse: 1.5648e-04\n",
      "Epoch 194/200\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 46ms/step - loss: 7.0330e-05 - mae: 0.0088 - mse: 1.4066e-04 - val_loss: 9.0507e-05 - val_mae: 0.0099 - val_mse: 1.8101e-04\n",
      "Epoch 195/200\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 45ms/step - loss: 7.6821e-05 - mae: 0.0091 - mse: 1.5364e-04 - val_loss: 7.6172e-05 - val_mae: 0.0088 - val_mse: 1.5234e-04\n",
      "Epoch 196/200\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 45ms/step - loss: 6.9880e-05 - mae: 0.0088 - mse: 1.3976e-04 - val_loss: 8.4055e-05 - val_mae: 0.0093 - val_mse: 1.6811e-04\n",
      "Epoch 197/200\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 47ms/step - loss: 7.2772e-05 - mae: 0.0089 - mse: 1.4554e-04 - val_loss: 1.1914e-04 - val_mae: 0.0117 - val_mse: 2.3828e-04\n",
      "Epoch 198/200\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 45ms/step - loss: 7.6506e-05 - mae: 0.0091 - mse: 1.5301e-04 - val_loss: 8.7402e-05 - val_mae: 0.0096 - val_mse: 1.7480e-04\n",
      "Epoch 199/200\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 46ms/step - loss: 7.2185e-05 - mae: 0.0089 - mse: 1.4437e-04 - val_loss: 7.6522e-05 - val_mae: 0.0088 - val_mse: 1.5304e-04\n",
      "Epoch 200/200\n",
      "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 45ms/step - loss: 6.8929e-05 - mae: 0.0086 - mse: 1.3786e-04 - val_loss: 7.5164e-05 - val_mae: 0.0088 - val_mse: 1.5033e-04\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - loss: 7.8835e-05 - mae: 0.0088 - mse: 1.5767e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[7.516390178352594e-05, 0.008778702467679977, 0.0001503278035670519]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Define callbacks and fit the model\n",
    "log_dir = \"../logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='loss', mode='min', patience=20)\n",
    "\n",
    "lr_scheduler = tf.keras.callbacks.LearningRateScheduler(schedule)\n",
    "\n",
    "model.fit(\n",
    "  x=x_train, \n",
    "  y=y_train,\n",
    "  epochs=200,\n",
    "  batch_size=64,\n",
    "  validation_data=(x_test, y_test),\n",
    "  callbacks=[tensorboard_callback, early_stopping,]) #Can add lr_scheduler\n",
    "\n",
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save the model\n",
    "model.save('../models/model_v2.keras')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
